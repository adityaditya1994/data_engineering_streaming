{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Local Spark with S3 Integration\n",
                "\n",
                "This notebook demonstrates how to:\n",
                "1. Load AWS credentials from a local JSON file\n",
                "2. Configure a local Spark session for S3 access\n",
                "3. Read sample clickstream data from S3\n",
                "4. Validate the connection and explore the data\n",
                "\n",
                "**Prerequisites:**\n",
                "- Java 8 or 11 installed (`java -version`)\n",
                "- PySpark installed (`pip install pyspark`)\n",
                "- AWS credentials file (see `aws_credentials_template.json`)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if needed\n",
                "# !pip install pyspark boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration - Update these paths for your environment\n",
                "CREDENTIALS_FILE = Path.home() / '.aws' / 'aws_credentials.json'\n",
                "# Alternative: use a path relative to this notebook\n",
                "# CREDENTIALS_FILE = Path('./aws_credentials.json')\n",
                "\n",
                "# S3 paths - Update for your buckets\n",
                "S3_BRONZE_BUCKET = 'your-bronze-bucket-name'\n",
                "S3_DATA_KEY = 'clickstream/sample_clickstream.json'\n",
                "\n",
                "print(f\"Credentials file: {CREDENTIALS_FILE}\")\n",
                "print(f\"S3 path: s3://{S3_BRONZE_BUCKET}/{S3_DATA_KEY}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load AWS Credentials from JSON File\n",
                "\n",
                "**Expected JSON format (`aws_credentials.json`):**\n",
                "```json\n",
                "{\n",
                "  \"aws_access_key_id\": \"YOUR_ACCESS_KEY_ID\",\n",
                "  \"aws_secret_access_key\": \"YOUR_SECRET_ACCESS_KEY\",\n",
                "  \"region\": \"eu-north-1\",\n",
                "  \"session_token\": null\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_aws_credentials(credentials_path: Path) -> dict:\n",
                "    \"\"\"\n",
                "    Load AWS credentials from a JSON file.\n",
                "    \n",
                "    Args:\n",
                "        credentials_path: Path to the JSON credentials file\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with AWS credentials\n",
                "    \n",
                "    Raises:\n",
                "        FileNotFoundError: If credentials file doesn't exist\n",
                "        ValueError: If required keys are missing\n",
                "    \"\"\"\n",
                "    if not credentials_path.exists():\n",
                "        raise FileNotFoundError(\n",
                "            f\"Credentials file not found: {credentials_path}\\n\"\n",
                "            f\"Create it using aws_credentials_template.json as a reference.\"\n",
                "        )\n",
                "    \n",
                "    with open(credentials_path, 'r') as f:\n",
                "        creds = json.load(f)\n",
                "    \n",
                "    required_keys = ['aws_access_key_id', 'aws_secret_access_key']\n",
                "    missing = [k for k in required_keys if not creds.get(k)]\n",
                "    \n",
                "    if missing:\n",
                "        raise ValueError(f\"Missing required credentials: {missing}\")\n",
                "    \n",
                "    print(f\"‚úÖ Loaded credentials for region: {creds.get('region', 'not specified')}\")\n",
                "    print(f\"   Access Key ID: {creds['aws_access_key_id'][:8]}...\")\n",
                "    \n",
                "    return creds\n",
                "\n",
                "# Load credentials\n",
                "try:\n",
                "    aws_creds = load_aws_credentials(CREDENTIALS_FILE)\n",
                "except FileNotFoundError as e:\n",
                "    print(f\"‚ö†Ô∏è  {e}\")\n",
                "    print(\"\\nFor testing, you can also use environment variables or ~/.aws/credentials\")\n",
                "    aws_creds = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Spark Session with S3 Configuration\n",
                "\n",
                "This configures Spark to use the Hadoop AWS library for S3 access."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "def create_spark_session_with_s3(credentials: dict = None) -> SparkSession:\n",
                "    \"\"\"\n",
                "    Create a local Spark session configured for S3 access.\n",
                "    \n",
                "    Args:\n",
                "        credentials: Dictionary with AWS credentials (optional if using IAM role or env vars)\n",
                "    \n",
                "    Returns:\n",
                "        Configured SparkSession\n",
                "    \"\"\"\n",
                "    builder = SparkSession.builder \\\n",
                "        .appName(\"LocalS3SparkSession\") \\\n",
                "        .master(\"local[*]\") \\\n",
                "        .config(\"spark.driver.memory\", \"2g\") \\\n",
                "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
                "        .config(\"spark.jars.packages\", \n",
                "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
                "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
                "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
                "    \n",
                "    # Add credentials configuration if provided\n",
                "    if credentials:\n",
                "        builder = builder \\\n",
                "            .config(\"spark.hadoop.fs.s3a.access.key\", credentials['aws_access_key_id']) \\\n",
                "            .config(\"spark.hadoop.fs.s3a.secret.key\", credentials['aws_secret_access_key'])\n",
                "        \n",
                "        if credentials.get('session_token'):\n",
                "            builder = builder \\\n",
                "                .config(\"spark.hadoop.fs.s3a.session.token\", credentials['session_token']) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
                "                        \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "        \n",
                "        if credentials.get('region'):\n",
                "            builder = builder \\\n",
                "                .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{credentials['region']}.amazonaws.com\")\n",
                "    else:\n",
                "        # Use default credential provider chain (env vars, ~/.aws/credentials, IAM role)\n",
                "        builder = builder \\\n",
                "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
                "                    \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
                "    \n",
                "    spark = builder.getOrCreate()\n",
                "    \n",
                "    print(f\"‚úÖ Spark session created\")\n",
                "    print(f\"   Version: {spark.version}\")\n",
                "    print(f\"   App ID: {spark.sparkContext.applicationId}\")\n",
                "    \n",
                "    return spark\n",
                "\n",
                "# Create Spark session\n",
                "spark = create_spark_session_with_s3(aws_creds)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Read Sample Data from S3\n",
                "\n",
                "Now let's read the sample clickstream data from your S3 bucket."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def read_json_from_s3(spark: SparkSession, bucket: str, key: str):\n",
                "    \"\"\"\n",
                "    Read JSON data from S3 into a Spark DataFrame.\n",
                "    \n",
                "    Args:\n",
                "        spark: Active SparkSession\n",
                "        bucket: S3 bucket name\n",
                "        key: S3 object key (path within bucket)\n",
                "    \n",
                "    Returns:\n",
                "        Spark DataFrame\n",
                "    \"\"\"\n",
                "    s3_path = f\"s3a://{bucket}/{key}\"\n",
                "    \n",
                "    print(f\"üìñ Reading from: {s3_path}\")\n",
                "    \n",
                "    try:\n",
                "        df = spark.read.json(s3_path)\n",
                "        record_count = df.count()\n",
                "        print(f\"‚úÖ Successfully read {record_count} records\")\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to read from S3: {e}\")\n",
                "        raise\n",
                "\n",
                "# Read data from S3\n",
                "# Uncomment when you have actual S3 bucket configured\n",
                "# df = read_json_from_s3(spark, S3_BRONZE_BUCKET, S3_DATA_KEY)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Alternative: Read Local Sample Data\n",
                "\n",
                "For testing without S3, you can read the local sample file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to local sample data (adjust as needed)\n",
                "LOCAL_SAMPLE_PATH = '../EMR/Pyspark/sample_clickstream.json'\n",
                "\n",
                "print(f\"üìñ Reading local file: {LOCAL_SAMPLE_PATH}\")\n",
                "\n",
                "df = spark.read.json(LOCAL_SAMPLE_PATH)\n",
                "\n",
                "print(f\"‚úÖ Loaded {df.count()} records from local file\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validate Schema and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display schema\n",
                "print(\"üìã DataFrame Schema:\")\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show sample records\n",
                "print(\"üìä Sample Records:\")\n",
                "df.show(truncate=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic statistics\n",
                "print(\"üìà Numeric Column Statistics:\")\n",
                "df.select('sensor_id', 'temperature', 'humidity').describe().show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Status distribution\n",
                "print(\"üîç Status Distribution:\")\n",
                "df.groupBy('status').count().show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Event type distribution\n",
                "print(\"üîç Event Type Distribution:\")\n",
                "df.groupBy('event_type').count().show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Validate Data Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, when, count, lit\n",
                "\n",
                "def validate_clickstream_data(df):\n",
                "    \"\"\"\n",
                "    Validate clickstream data against expected quality rules.\n",
                "    \n",
                "    Returns a summary of validation results.\n",
                "    \"\"\"\n",
                "    validations = {\n",
                "        'sensor_id_positive': (col('sensor_id') > 0),\n",
                "        'temp_in_range': (col('temperature').between(-50, 100)),\n",
                "        'humidity_in_range': (col('humidity').between(0, 100)),\n",
                "        'valid_status': col('status').isin('active', 'faulty', 'inactive'),\n",
                "        'timestamp_positive': (col('timestamp') > 0)\n",
                "    }\n",
                "    \n",
                "    total = df.count()\n",
                "    print(f\"üìä Data Quality Validation (Total Records: {total})\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    for rule_name, condition in validations.items():\n",
                "        valid_count = df.filter(condition).count()\n",
                "        pct = (valid_count / total) * 100 if total > 0 else 0\n",
                "        status = \"‚úÖ\" if pct == 100 else \"‚ö†Ô∏è\"\n",
                "        print(f\"{status} {rule_name}: {valid_count}/{total} ({pct:.1f}%)\")\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "\n",
                "validate_clickstream_data(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test S3 Write (Optional)\n",
                "\n",
                "If you want to test writing back to S3:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to test S3 write\n",
                "# OUTPUT_PATH = f\"s3a://{S3_BRONZE_BUCKET}/test_output/\"\n",
                "# \n",
                "# df.write \\\n",
                "#     .mode(\"overwrite\") \\\n",
                "#     .parquet(OUTPUT_PATH)\n",
                "# \n",
                "# print(f\"‚úÖ Data written to: {OUTPUT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark session when done\n",
                "# spark.stop()\n",
                "# print(\"‚úÖ Spark session stopped\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Troubleshooting\n",
                "\n",
                "### Common Issues\n",
                "\n",
                "1. **Java not found**: Install Java 8 or 11 and set `JAVA_HOME`\n",
                "2. **S3 access denied**: Check your AWS credentials and bucket permissions\n",
                "3. **Slow first run**: Spark downloads required JARs on first run (cached after)\n",
                "4. **Memory errors**: Increase `spark.driver.memory` in session config\n",
                "\n",
                "### Credential Priority\n",
                "\n",
                "Spark checks credentials in this order:\n",
                "1. Explicit configuration (from JSON file)\n",
                "2. Environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`)\n",
                "3. AWS CLI config (`~/.aws/credentials`)\n",
                "4. IAM instance role (on EC2)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}