{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Local Spark with S3 Integration (Fixed)\n",
                "\n",
                "This notebook demonstrates how to:\n",
                "1. Load AWS credentials from Environment Variables (Docker) or local JSON file\n",
                "2. Configure a local Spark session for S3 access\n",
                "3. Read sample clickstream data from S3\n",
                "4. Validate the connection and explore the data\n",
                "\n",
                "**Prerequisites:**\n",
                "- Java 8 or 11 installed (`java -version`)\n",
                "- PySpark installed (`pip install pyspark`)\n",
                "- AWS credentials (Env Vars or file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if needed\n",
                "# !pip install pyspark boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration - Update these paths for your environment\n",
                "CREDENTIALS_FILE = Path.home() / '.aws' / 'aws_credentials.json'\n",
                "# Alternative: use a path relative to this notebook\n",
                "# CREDENTIALS_FILE = Path('./aws_credentials.json')\n",
                "\n",
                "# S3 paths - Update for your buckets\n",
                "S3_BRONZE_BUCKET = os.environ.get('SCRIPT_BUCKET', 'your-bronze-bucket-name').replace('-scripts-', '-bronze-')\n",
                "S3_DATA_KEY = 'clickstream/sample_clickstream.json'\n",
                "\n",
                "print(f\"Credentials file: {CREDENTIALS_FILE}\")\n",
                "print(f\"S3 path: s3://{S3_BRONZE_BUCKET}/{S3_DATA_KEY}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load AWS Credentials\n",
                "\n",
                "Prioritizes Environment Variables (injected by Docker) over local files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_aws_credentials(credentials_path: Path) -> dict:\n",
                "    \"\"\"\n",
                "    Load AWS credentials from Environment Variables or JSON file.\n",
                "    Priority: Env Vars > JSON File\n",
                "    \"\"\"\n",
                "    # 1. Check Environment Variables (Docker injection)\n",
                "    if os.environ.get('AWS_ACCESS_KEY_ID') and os.environ.get('AWS_SECRET_ACCESS_KEY'):\n",
                "        print(\"‚úÖ Loaded credentials from Environment Variables\")\n",
                "        return {\n",
                "            'aws_access_key_id': os.environ['AWS_ACCESS_KEY_ID'],\n",
                "            'aws_secret_access_key': os.environ['AWS_SECRET_ACCESS_KEY'],\n",
                "            'region': os.environ.get('AWS_DEFAULT_REGION', 'eu-north-1'),\n",
                "            'session_token': os.environ.get('AWS_SESSION_TOKEN')\n",
                "        }\n",
                "    \n",
                "    # 2. Check JSON File\n",
                "    if not credentials_path.exists():\n",
                "        # If not found and no env vars, return None (let Spark try default chain)\n",
                "        print(f\"‚ö†Ô∏è  Credentials file not found: {credentials_path}\")\n",
                "        print(\"   Will attempt to use default chain (e.g. ~/.aws/credentials if mounted)\")\n",
                "        return None\n",
                "    \n",
                "    with open(credentials_path, 'r') as f:\n",
                "        creds = json.load(f)\n",
                "    \n",
                "    print(f\"‚úÖ Loaded credentials from JSON file\")\n",
                "    return creds\n",
                "\n",
                "# Load credentials\n",
                "aws_creds = load_aws_credentials(CREDENTIALS_FILE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Spark Session with S3 Configuration\n",
                "\n",
                "This configures Spark to use the Hadoop AWS library for S3 access."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "def create_spark_session_with_s3(credentials: dict = None) -> SparkSession:\n",
                "    \"\"\"\n",
                "    Create a local Spark session configured for S3 access.\n",
                "    \n",
                "    Args:\n",
                "        credentials: Dictionary with AWS credentials (optional if using IAM role or env vars)\n",
                "    \n",
                "    Returns:\n",
                "        Configured SparkSession\n",
                "    \"\"\"\n",
                "    builder = SparkSession.builder \\\n",
                "        .appName(\"LocalS3SparkSession\") \\\n",
                "        .master(\"local[*]\") \\\n",
                "        .config(\"spark.driver.memory\", \"2g\") \\\n",
                "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
                "        .config(\"spark.jars.packages\", \n",
                "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
                "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
                "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
                "        .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\") \\\n",
                "        .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\")\n",
                "    \n",
                "    # Add credentials configuration if provided\n",
                "    if credentials:\n",
                "        builder = builder \\\n",
                "            .config(\"spark.hadoop.fs.s3a.access.key\", credentials['aws_access_key_id']) \\\n",
                "            .config(\"spark.hadoop.fs.s3a.secret.key\", credentials['aws_secret_access_key'])\n",
                "        \n",
                "        if credentials.get('session_token'):\n",
                "            builder = builder \\\n",
                "                .config(\"spark.hadoop.fs.s3a.session.token\", credentials['session_token']) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
                "                        \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "        \n",
                "        if credentials.get('region'):\n",
                "            builder = builder \\\n",
                "                .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{credentials['region']}.amazonaws.com\")\n",
                "    else:\n",
                "        # Use default credential provider chain (env vars, ~/.aws/credentials, IAM role)\n",
                "        builder = builder \\\n",
                "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
                "                    \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
                "    \n",
                "    spark = builder.getOrCreate()\n",
                "    \n",
                "    print(f\"‚úÖ Spark session created\")\n",
                "    print(f\"   Version: {spark.version}\")\n",
                "    print(f\"   App ID: {spark.sparkContext.applicationId}\")\n",
                "    \n",
                "    return spark\n",
                "\n",
                "# Create Spark session\n",
                "spark = create_spark_session_with_s3(aws_creds)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Read Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "s3_path = f\"s3a://{S3_BRONZE_BUCKET}/{S3_DATA_KEY}\"\n",
                "print(f\"üìñ Reading from: {s3_path}\")\n",
                "\n",
                "try:\n",
                "    df = spark.read.json(s3_path)\n",
                "    print(f\"‚úÖ Successfully read {df.count()} records\")\n",
                "    df.show()\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Failed to read from S3: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}